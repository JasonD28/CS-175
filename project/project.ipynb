{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSi3wsC_kDal",
        "colab_type": "text"
      },
      "source": [
        "# Final Project Report for CS 175, Spring 2020\n",
        "**Project Title:** Water Filled Lung Detector\n",
        "\n",
        "**Project Number:** Group 14\n",
        "\n",
        "**Student Name(s)**\n",
        "\n",
        "Allan Tran, 61735904. allannt@uci.edu\n",
        "\n",
        "Jason Davis, 22336416, jasonbd@uci.edu\n",
        "\n",
        "Eva Dai, 94015611, ydai8@uci.edu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRNXMy1akDan",
        "colab_type": "text"
      },
      "source": [
        "First, load any source code we need and import them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IurQ3yGqkDao",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "6e97afae-1cd7-4215-eab0-c179606a290b"
      },
      "source": [
        "%%bash\n",
        "\n",
        "# Using PyTorch helper functions to simplify training\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "cd vision\n",
        "git checkout v0.5.1\n",
        "\n",
        "cp references/detection/utils.py ../\n",
        "cp references/detection/engine.py ../\n",
        "cp references/detection/transforms.py ../\n",
        "cp references/detection/coco_eval.py ../\n",
        "cp references/detection/coco_utils.py ../"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Branch 'v0.5.1' set up to track remote branch 'v0.5.1' from 'origin'.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'vision'...\n",
            "Switched to a new branch 'v0.5.1'\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvDz0rQtlZUU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "2f7f79a3-03a3-4b0b-dbc1-fb17363307ee"
      },
      "source": [
        "!pip install pydicom"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pydicom\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/56/342e1f8ce5afe63bf65c23d0b2c1cd5a05600caad1c211c39725d3a4cc56/pydicom-2.0.0-py3-none-any.whl (35.4MB)\n",
            "\u001b[K     |████████████████████████████████| 35.5MB 90kB/s \n",
            "\u001b[?25hInstalling collected packages: pydicom\n",
            "Successfully installed pydicom-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLPdq_2Jsr1T",
        "colab_type": "text"
      },
      "source": [
        "### **Warning: The most recent version of pycocotools from https://github.com/cocodataset/cocoapi.git is needed to import the modules in the first import cell and to run the Faster RCNN part of this notebook**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cInTcgd_thM6",
        "colab_type": "text"
      },
      "source": [
        "The next code block attempts to update the pycocotools library on your computer if you already have it installed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KFOTqkFuHU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "\n",
        "git clone https://github.com/cocodataset/cocoapi.git\n",
        "cd cocoapi\n",
        "cp PythonAPI/pycocotools C:/Python36/Lib/site-packages/ -r"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Zg-jQYNsolH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from engine import train_one_epoch, evaluate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dijiZdpvBDmj",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import utils\n",
        "from torch import optim\n",
        "import torchvision\n",
        "from src.dataset import PneumoniaDataset\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from src.classifier import Classifier\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import sampler\n",
        "from torch.nn.functional import relu as Relu\n",
        "from torch import sigmoid\n",
        "\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import timeit\n",
        "\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pg6xyqQkDax",
        "colab_type": "text"
      },
      "source": [
        "## Classifier:\n",
        "\n",
        "Here we start to load the classifier model and train it with a small sample of our data for demostration purpose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ag3sOE85kDay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "root = \"./data\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CZXp7EckDa4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "6585df02-2342-4365-cb6b-d39d55eb444f"
      },
      "source": [
        "model = Classifier();\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classifier(\n",
              "  (convInput): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv32): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv32to64): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv64): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv64to128): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv128): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (pooling): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (linearto128): Linear(in_features=2097152, out_features=128, bias=True)\n",
              "  (linearto1): Linear(in_features=128, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZKnXANEkDa7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = PneumoniaDataset(root, True)\n",
        "validation_dataset = PneumoniaDataset(root, True)\n",
        "indices = torch.randperm(len(dataset)).tolist()\n",
        "dataset = torch.utils.data.Subset(dataset, indices[:20])\n",
        "validation_dataset = torch.utils.data.Subset(validation_dataset, indices[20:])\n",
        "\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=1, shuffle=True, num_workers=1,\n",
        "        collate_fn=utils.collate_fn)\n",
        "\n",
        "validation_data_loader = torch.utils.data.DataLoader(\n",
        "    validation_dataset, batch_size=1, shuffle=False, num_workers=2,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "\n",
        "test_dataset = PneumoniaDataset(root, True, 'test')\n",
        "\n",
        "test_data_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset, batch_size=1, shuffle=False, num_workers=2,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "gpu_dtype = torch.cuda.FloatTensor\n",
        "loss_fn = nn.BCEWithLogitsLoss().type(gpu_dtype)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCy9Zi9bkDa-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(train_data, dtype, model, loss_fn, optimizer, num_epochs=1):\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Starting epoch %d / %d' % (epoch + 1, num_epochs))\n",
        "        model.train()\n",
        "        for t, (image, target) in enumerate(train_data):\n",
        "            imgs = torch.tensor([img.numpy() for img in image], dtype=torch.float32)\n",
        "            x_var = Variable(imgs.type(dtype))\n",
        "            l = []\n",
        "            for tar in target:\n",
        "              if len(tar['labels']) > 1:\n",
        "                # temp = [list(tar['labels'])[0]]\n",
        "                l.append([1])\n",
        "              else:\n",
        "                if tar['labels'] == 2:\n",
        "                  l.append([1])\n",
        "                else:\n",
        "                  l.append([0])\n",
        "            y_var = torch.tensor(l).cuda() # Variable(labels.type(dtype).cuda())\n",
        "            scores = model(imgs.cuda())\n",
        "            loss = loss_fn(scores, y_var.float())\n",
        "            if (t + 1) % 5 == 0:\n",
        "              print('t = %d, loss = %.4f' % (t + 1, loss.item()))\n",
        "              print(y_var)\n",
        "              print(scores)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3CMVI4pkDbB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "91c68dba-4b1d-44a6-fdc6-6e962aa6bff4"
      },
      "source": [
        "train(data_loader, gpu_dtype, model, loss_fn, optimizer, 1)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting epoch 1 / 1\n",
            "t = 5, loss = 0.0000\n",
            "tensor([[0]], device='cuda:0')\n",
            "tensor([[-61.6630]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "t = 10, loss = 0.0000\n",
            "tensor([[0]], device='cuda:0')\n",
            "tensor([[-11.3769]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "t = 15, loss = 1.5481\n",
            "tensor([[0]], device='cuda:0')\n",
            "tensor([[1.3090]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "t = 20, loss = 0.5898\n",
            "tensor([[0]], device='cuda:0')\n",
            "tensor([[-0.2187]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oM8Zqg8LkDbE",
        "colab_type": "text"
      },
      "source": [
        "After the training, we can check the accuracy on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHr3Zi-LkDbE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_accuracy(model, loader):\n",
        "    \"\"\"if loader.dataset.train:\n",
        "        print('Checking accuracy on validation set')\n",
        "    else:\n",
        "        print('Checking accuracy on test set')\"\"\"  \n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval() # Put the model in test mode (the opposite of model.train(), essentially)\n",
        "    for t, (image, target) in enumerate(loader):\n",
        "        with torch.no_grad():\n",
        "          imgs = torch.tensor([img.numpy() for img in image], dtype=torch.float32)\n",
        "          l = []\n",
        "          metadata = []\n",
        "          for tar in target:\n",
        "            if len(tar['labels']) > 1:\n",
        "              l.append([1])\n",
        "            else:\n",
        "              if tar['labels'] == 2:\n",
        "                l.append([1])\n",
        "              else:\n",
        "                l.append([0])\n",
        "            metadata.append([tar['position']])\n",
        "          l = torch.tensor(l)\n",
        "        raw_scores = sigmoid(model(imgs.cuda()))\n",
        "        scores = torch.tensor([int(raw_scores > 0.37)])\n",
        "        num_correct += (scores == l).sum()\n",
        "        num_samples += scores.size(0)\n",
        "        if (t + 1) % 1 == 0:\n",
        "            print('t = %d, num_correct = %d, num_samples = %d' % (t + 1, num_correct, num_samples))\n",
        "    acc = float(num_correct) / num_samples\n",
        "    print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZ7weVCbkDbJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "0f8f82f9-2699-4358-a6f7-1e4cde09f372"
      },
      "source": [
        "check_accuracy(model, validation_data_loader)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t = 1, num_correct = 0, num_samples = 1\n",
            "t = 2, num_correct = 0, num_samples = 2\n",
            "t = 3, num_correct = 0, num_samples = 3\n",
            "t = 4, num_correct = 0, num_samples = 4\n",
            "t = 5, num_correct = 1, num_samples = 5\n",
            "t = 6, num_correct = 2, num_samples = 6\n",
            "Got 2 / 6 correct (33.33)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt-iNxDEkDbM",
        "colab_type": "text"
      },
      "source": [
        "Now we can save the predictions into a csv file. But for demostration, here we'll print out the predictions instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVrqmghNkDbN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_preds(model, loader, dataset):\n",
        "    model.eval() # Put the model in test mode (the opposite of model.train(), essentially)\n",
        "    pIds = []\n",
        "    predictions = []\n",
        "    for index, (image, target) in enumerate(loader):\n",
        "        with torch.no_grad():\n",
        "          imgs = torch.tensor([img.numpy() for img in image], dtype=torch.float32)\n",
        "        scores =  torch.tensor([int(sigmoid(model(imgs.cuda())) > 0.37)])\n",
        "        patientId = dataset.imgs[index][:-4]\n",
        "        p = scores.numpy()[0]\n",
        "        pIds.append(patientId)\n",
        "        predictions.append(p)\n",
        "\n",
        "    d = {'patientId': pIds, 'pred': predictions}\n",
        "    #df = pd.DataFrame(data=d)\n",
        "    #path = 'drive/My Drive/cs-175-project/predictions/classifier_w_metadata_prediction.csv'\n",
        "    #df.to_csv(path, index=False)\n",
        "    print(d)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6jI4MB7kDbQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "784a2d2a-6c6f-4850-ad12-b85bc87d1752"
      },
      "source": [
        "save_preds(model, test_data_loader, test_dataset)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'patientId': ['0000a175-0e68-4ca4-b1af-167204a7e0bc', '0005d3cc-3c3f-40b9-93c3-46231c3eb813', '000686d7-f4fc-448d-97a0-44fa9c5d3aa6', '000e3a7d-c0ca-4349-bb26-5af2d8993c3d', '00100a24-854d-423d-a092-edcf6179e061', '0015597f-2d69-4bc7-b642-5b5e01534676', '001b0c51-c7b3-45c1-9c17-fa7594cab96e', '0022bb50-bf6c-4185-843e-403a9cc1ea80', '00271e8e-aea8-4f0a-8a34-3025831f1079', '0028450f-5b8e-4695-9416-8340b6f686b0'], 'pred': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OyA2rtukDbT",
        "colab_type": "text"
      },
      "source": [
        "As shown in the validation and test predictions above, we can see the accuracy is very low. It is caused by the small number of the training data we have in this submission. During the actual project, we ran on the entire dataset of 26k images, which would result in a much higher accuracy of 90.1%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0msGoMa-li9J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wy37xxkolmfi",
        "colab_type": "text"
      },
      "source": [
        "## Faster RCNN:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K68S56m-loRL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d9fb8e87-fa15-4c48-9e73-e2ab239b4575"
      },
      "source": [
        "faster_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "in_features = faster_model.roi_heads.box_predictor.cls_score.in_features\n",
        "faster_model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes=3)\n",
        "faster_model.to(device)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FasterRCNN(\n",
              "  (transform): GeneralizedRCNNTransform(\n",
              "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
              "  )\n",
              "  (backbone): BackboneWithFPN(\n",
              "    (body): IntermediateLayerGetter(\n",
              "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "      (bn1): FrozenBatchNorm2d()\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      (layer1): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d()\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d()\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d()\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d()\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d()\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d()\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d()\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d()\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d()\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d()\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer2): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d()\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d()\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d()\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d()\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d()\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d()\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d()\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d()\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d()\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d()\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d()\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d()\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d()\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer3): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d()\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d()\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d()\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d()\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d()\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d()\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d()\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d()\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d()\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d()\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d()\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d()\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d()\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (4): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d()\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d()\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d()\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (5): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d()\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d()\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d()\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer4): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d()\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d()\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d()\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d()\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d()\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d()\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d()\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d()\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d()\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d()\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (fpn): FeaturePyramidNetwork(\n",
              "      (inner_blocks): ModuleList(\n",
              "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "      (layer_blocks): ModuleList(\n",
              "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "      (extra_blocks): LastLevelMaxPool()\n",
              "    )\n",
              "  )\n",
              "  (rpn): RegionProposalNetwork(\n",
              "    (anchor_generator): AnchorGenerator()\n",
              "    (head): RPNHead(\n",
              "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (roi_heads): RoIHeads(\n",
              "    (box_roi_pool): MultiScaleRoIAlign()\n",
              "    (box_head): TwoMLPHead(\n",
              "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
              "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    )\n",
              "    (box_predictor): FastRCNNPredictor(\n",
              "      (cls_score): Linear(in_features=1024, out_features=3, bias=True)\n",
              "      (bbox_pred): Linear(in_features=1024, out_features=12, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oi-KQEF8lreC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "ff5008e7-919a-4cce-b93f-163a8c7ce268"
      },
      "source": [
        "train_one_epoch(faster_model, optimizer, data_loader, device, 1, 5)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
            "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:\n",
            "\tnonzero(Tensor input, *, Tensor out)\n",
            "Consider using one of the following signatures instead:\n",
            "\tnonzero(Tensor input, *, bool as_tuple)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: [1]  [ 0/20]  eta: 0:00:06  lr: 0.001000  loss: 3.8248 (3.8248)  loss_classifier: 1.3701 (1.3701)  loss_box_reg: 0.0010 (0.0010)  loss_objectness: 1.7421 (1.7421)  loss_rpn_box_reg: 0.7116 (0.7116)  time: 0.3299  data: 0.1138  max mem: 6475\n",
            "Epoch: [1]  [ 5/20]  eta: 0:00:03  lr: 0.001000  loss: 3.7450 (3.0420)  loss_classifier: 1.3701 (1.3862)  loss_box_reg: 0.0004 (0.0070)  loss_objectness: 1.6429 (1.1531)  loss_rpn_box_reg: 0.7116 (0.4957)  time: 0.2167  data: 0.0235  max mem: 6475\n",
            "Epoch: [1]  [10/20]  eta: 0:00:02  lr: 0.001000  loss: 3.7659 (3.2419)  loss_classifier: 1.3747 (1.3778)  loss_box_reg: 0.0010 (0.0111)  loss_objectness: 1.6821 (1.3136)  loss_rpn_box_reg: 0.7254 (0.5393)  time: 0.2063  data: 0.0153  max mem: 6475\n",
            "Epoch: [1]  [15/20]  eta: 0:00:01  lr: 0.001000  loss: 3.7659 (3.3336)  loss_classifier: 1.3747 (1.3791)  loss_box_reg: 0.0010 (0.0123)  loss_objectness: 1.6776 (1.3848)  loss_rpn_box_reg: 0.7254 (0.5573)  time: 0.2023  data: 0.0121  max mem: 6475\n",
            "Epoch: [1]  [19/20]  eta: 0:00:00  lr: 0.001000  loss: 3.7659 (3.3269)  loss_classifier: 1.3747 (1.3799)  loss_box_reg: 0.0010 (0.0111)  loss_objectness: 1.6776 (1.3787)  loss_rpn_box_reg: 0.7266 (0.5572)  time: 0.2004  data: 0.0108  max mem: 6475\n",
            "Epoch: [1] Total time: 0:00:04 (0.2026 s / it)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<utils.MetricLogger at 0x7fbd64b01da0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZedvnClnSvU",
        "colab_type": "text"
      },
      "source": [
        "This code uses a PyTorch helper function to train the Faster RCNN Model on the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwBdoJTal7ML",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "7a2ed889-8dba-45be-df7c-d6b07ba28288"
      },
      "source": [
        "evaluate(faster_model, validation_data_loader, device)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "creating index...\n",
            "index created!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test:  [0/6]  eta: 0:00:01  model_time: 0.0681 (0.0681)  evaluator_time: 0.0033 (0.0033)  time: 0.2196  data: 0.1449  max mem: 6475\n",
            "Test:  [5/6]  eta: 0:00:00  model_time: 0.0606 (0.0617)  evaluator_time: 0.0033 (0.0053)  time: 0.0974  data: 0.0287  max mem: 6475\n",
            "Test: Total time: 0:00:00 (0.1087 s / it)\n",
            "Averaged stats: model_time: 0.0606 (0.0617)  evaluator_time: 0.0033 (0.0053)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.01s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.001\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.001\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.025\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.075\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<coco_eval.CocoEvaluator at 0x7fbdc0746128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0UIro_SoBB8",
        "colab_type": "text"
      },
      "source": [
        "We also used a PyTorch helper function to evaluate the model on the validation dataset. Because of the small datasize and using only 1 epoch, the top line showing the mean average precision of our predicted boxes with confidence between 50% and 95% is pretty low, \"Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Olf9DZOmw7d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "faster_preds = pd.DataFrame(columns = ['patientId', 'PredictionString'])\n",
        "confidence = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYuKL2tPpWOv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_faster_preds(model, data, testset, device, conf, dataframe):\n",
        "  model.eval()\n",
        "  predictions = []\n",
        "  for i, (x, y) in enumerate(data):\n",
        "    with torch.no_grad():\n",
        "      images = list(image.to(device) for image in x)\n",
        "    preds = model(images)[0]\n",
        "    pred_str = []\n",
        "    for index, score in enumerate(preds['scores'].tolist()):\n",
        "      if score >= conf:\n",
        "        boxes = preds['boxes'][index].tolist()\n",
        "        if preds['labels'][index] == 2:\n",
        "          pred_str.extend([score, boxes[0], boxes[1], boxes[2] - boxes[0], boxes[3] - boxes[1]])\n",
        "    pred_str = [str(x) for x in pred_str]\n",
        "    pred_str = ' '.join(pred_str)\n",
        "    dataframe = dataframe.append({'patientId': testset.imgs[i][:-4], 'PredictionString': pred_str}, ignore_index=True)\n",
        "  return dataframe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyX3i1pmp2kt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "b93bbe9c-cf15-41ca-f70c-e5ce30c4469f"
      },
      "source": [
        "final_preds = get_faster_preds(faster_model, test_data_loader, test_dataset, device, confidence, faster_preds)\n",
        "print(final_preds)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                              patientId                                   PredictionString\n",
            "0  0000a175-0e68-4ca4-b1af-167204a7e0bc  0.5763124227523804 0.0 0.772656261920929 328.2...\n",
            "1  0005d3cc-3c3f-40b9-93c3-46231c3eb813  0.6492416858673096 0.0 15.478055953979492 247....\n",
            "2  000686d7-f4fc-448d-97a0-44fa9c5d3aa6  0.632843554019928 876.880859375 99.17414855957...\n",
            "3  000e3a7d-c0ca-4349-bb26-5af2d8993c3d  0.5587719678878784 0.0 0.0 196.31295776367188 ...\n",
            "4  00100a24-854d-423d-a092-edcf6179e061  0.6293582916259766 772.3051147460938 0.0 251.6...\n",
            "5  0015597f-2d69-4bc7-b642-5b5e01534676  0.6138786673545837 785.5523071289062 159.37661...\n",
            "6  001b0c51-c7b3-45c1-9c17-fa7594cab96e  0.585543155670166 349.5295715332031 0.0 345.61...\n",
            "7  0022bb50-bf6c-4185-843e-403a9cc1ea80  0.6658321619033813 84.30679321289062 65.343292...\n",
            "8  00271e8e-aea8-4f0a-8a34-3025831f1079  0.6860902309417725 918.9534301757812 99.247772...\n",
            "9  0028450f-5b8e-4695-9416-8340b6f686b0  0.6256241798400879 809.3922729492188 52.985641...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wm9cu0-5rRaq",
        "colab_type": "text"
      },
      "source": [
        "Here we can see the output of the model in the of the Kaggle Competition result format. One patient id associated with a string of the confidence of the model for a bounding box, the x and y coordinate of the box, and its width and height"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-vgXAXKsLqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}